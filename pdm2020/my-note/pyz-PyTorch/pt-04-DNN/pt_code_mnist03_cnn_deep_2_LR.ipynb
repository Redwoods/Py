{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch MNIST\n",
    "- deep CNN\n",
    "\n",
    "> https://github.com/pytorch/examples/blob/main/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU 사용 가능하면 사용하고 아니면 CPU 사용\n",
    "print(\"다음 기기로 학습합니다:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\life21c\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "mnist_train = datasets.MNIST(root='../../MNIST_data/', # 다운로드 경로 지정\n",
    "                          train=True, # True를 지정하면 훈련 데이터로 다운로드\n",
    "                          transform=transform, # 텐서로 변환\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = datasets.MNIST(root='../../MNIST_data/', # 다운로드 경로 지정\n",
    "                         train=False, # False를 지정하면 테스트 데이터로 다운로드\n",
    "                         transform=transform, # 텐서로 변환\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]), torch.Size([10000, 28, 28]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.data.shape,mnist_test.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "# def train(args, model, device, train_loader, optimizer, epoch):\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         loss = F.nll_loss(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if batch_idx % args.log_interval == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "#             if args.dry_run:\n",
    "#                 break\n",
    "\n",
    "\n",
    "# def test(model, device, test_loader):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "#             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset),\n",
    "#         100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 정의\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 1.0  #0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 정의\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 배치의 수 : 600\n"
     ]
    }
   ],
   "source": [
    "# 총 배치의 수를 출력해보겠습니다.\n",
    "\n",
    "total_batch = len(train_loader)\n",
    "print('총 배치의 수 : {}'.format(total_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            # if args.dry_run:\n",
    "            #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start :\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.299644\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.266337\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.096404\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.150625\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.122506\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.108734\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.012713\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.187948\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.038193\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.127332\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.117500\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.037789\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.049422\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.052618\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.118299\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.028719\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.047243\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.024964\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.012264\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.021325\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.028260\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.024021\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.018024\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.064248\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.029236\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.019991\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.021202\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.093212\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.097752\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.014405\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.052636\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.093876\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.076130\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.021090\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.033419\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.026757\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.036051\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.009237\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.024511\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.023562\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.010403\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.022635\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.003689\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.028101\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.009599\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.013309\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.101353\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.118468\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.019569\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.009594\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.016791\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.002544\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.008999\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.017642\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.015978\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.057978\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.002676\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.073735\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.020700\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.004487\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.056108\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.091838\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.007256\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.003391\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.001761\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.014752\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.004134\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 0.071108\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 0.030982\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 0.000768\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 0.083584\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 0.006013\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.044942\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 0.001502\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 0.005378\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 0.010054\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.034851\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 0.001587\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.027555\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 0.011641\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 0.011363\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 0.049433\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 0.001878\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 0.001150\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.008929\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 0.027760\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 0.002149\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 0.000192\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 0.003526\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 0.001129\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.002451\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 0.001790\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 0.061045\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 0.000076\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.014688\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 0.001514\n",
      "Learning finished\n",
      "Training Executed in 136.62104119999998 sec\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "print(\"Training start :\")\n",
    "# Training\n",
    "for epoch in range(training_epochs+1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "\n",
    "print('Learning finished')\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "execution_time = stop - start\n",
    "\n",
    "print(\"Training Executed in \" + str(execution_time) + \" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing start :\n",
      "\n",
      "Test set: Average loss: 0.0368, Accuracy: 9924/10000 (99%)\n",
      "\n",
      "Learning finished\n",
      "Tesing Executed in 1.321959899999996 sec\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "print(\"Testing start :\")\n",
    "# Training\n",
    "test(model, device, test_loader)\n",
    "\n",
    "print('Learning finished')\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "execution_time = stop - start\n",
    "\n",
    "print(\"Tesing Executed in \" + str(execution_time) + \" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9710999727249146\n",
      "Label:  5\n",
      "Prediction:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAANxklEQVR4nO3dfahc9Z3H8c/HxBh8xJhrCFaMW/xjdaO2DrqgFpe6Pv2jRbrWh/rMLWLUgiGaLlIVlLBsWxcNkvhA49K1VDSoILuVWJSKFEdxTTQ+ZE3SJobkSpBqMLjqd/+4x+Wa3PnNzcyZB+73/YJhZs53zpwvh3xyZs7v3Pk5IgRg+ttv0A0A6A/CDiRB2IEkCDuQBGEHkpjZz43NnTs3FixY0M9NAqls2rRJH330kSerdRV22+dJ+jdJMyQ9HBHLSq9fsGCBms1mN5sEUNBoNFrWOv4Yb3uGpOWSzpd0vKRLbR/f6fsB6K1uvrOfKmlDRHwQEZ9L+q2kC+tpC0Ddugn7UZL+MuH5lmrZN9getd203RwbG+ticwC60U3YJzsJsNe1txGxMiIaEdEYGRnpYnMAutFN2LdIOnrC829J+rC7dgD0Sjdhf1XScbaPtT1L0o8kPVNPWwDq1vHQW0R8YXuRpP/S+NDboxHxVm2dAahVV+PsEfGcpOdq6gVAD3G5LJAEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ9HXKZkw/mzdvLtaXLFnSsvbEE090te0TTzyxWL/sssta1m655ZbiugcccEBHPQ0zjuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7Cj69NNPi/UzzzyzWN+6dWvLmu2Oevra2rVri/WlS5e2rJ100knFdc8999yOehpmXYXd9iZJn0j6UtIXEdGooykA9avjyP4PEfFRDe8DoIf4zg4k0W3YQ9Lvbb9me3SyF9getd203RwbG+tycwA61W3YT4+I70o6X9KNtr+35wsiYmVENCKiMTIy0uXmAHSqq7BHxIfV/Q5JqyWdWkdTAOrXcdhtH2T7kK8fSzpH0rq6GgNQr27Oxs+TtLoaK50p6T8i4j9r6QpD47bbbivWS+PoGC4dhz0iPpBUvjIBwNBg6A1IgrADSRB2IAnCDiRB2IEk+BPXaW737t3F+uLFi4v1Bx98sFhv95PLzz77bMva/Pnzi+s+9dRTxfqKFSuK9VmzZrWsLVy4sLjudMSRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJx9mrvyyiuL9SeffLJYnz17drG+evXqYv3ss88u1ktOOOGEYv2OO+7o+L0z4sgOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzj4N7Ny5s2VtzZo1Xb33PffcU6xPx6mNpyuO7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPs08CuXbta1j7++OOu3nvjxo3F+ueff16s77///i1r1XTf6JO2R3bbj9reYXvdhGVzbD9v+/3q/vDetgmgW1P5GP9rSeftsex2SWsi4jhJa6rnAIZY27BHxEuS9rwe80JJq6rHqyRdVG9bAOrW6Qm6eRGxTZKq+yNbvdD2qO2m7ebY2FiHmwPQrZ6fjY+IlRHRiIjGyMhIrzcHoIVOw77d9nxJqu531NcSgF7oNOzPSLqqenyVpKfraQdAr7QdZ7f9uKSzJM21vUXSzyUtk/Q729dJ+rOkH/aySZRt3ry5Z+/9wAMPFOvLly8v1u+///6WtRtuuKG4LuPw9Wob9oi4tEXp+zX3AqCHuFwWSIKwA0kQdiAJwg4kQdiBJPgT12lg4cKFLWvz5s0rrrt9+/a62/mGm266qWWt3dBau6E57BuO7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPs08Bhhx3Wsvbee+8V133ssceK9UWLFnXU01TcfPPNxfqMGTOK9dHR0TrbmfY4sgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEo6Ivm2s0WhEs9ns2/bQvd27dxfry5YtK9bvvvvujrd9xhlnFOsvvPBCsT5zZr7LSBqNhprN5qQ/FMCRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSyDcQiX0ye/bsYn3JkiXF+tjYWMvaypUri+u+/PLLxfqWLVuK9QULFhTr2bQ9stt+1PYO2+smLLvT9lbbb1S3C3rbJoBuTeVj/K8lnTfJ8l9FxMnV7bl62wJQt7Zhj4iXJO3sQy8AeqibE3SLbL9Zfcw/vNWLbI/abtpulr6/AeitTsP+oKRvSzpZ0jZJv2j1wohYGRGNiGiMjIx0uDkA3eoo7BGxPSK+jIivJD0k6dR62wJQt47Cbnv+hKc/kLSu1WsBDIe24+y2H5d0lqS5trdI+rmks2yfLCkkbZL0k961iGF24IEHFuvLly9vWZszZ05x3XvvvbejnjC5tmGPiEsnWfxID3oB0ENcLgskQdiBJAg7kARhB5Ig7EAS/Ikreuqzzz5rWVu3rnx5xrHHHlusH3HEER31lBVHdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2GmzevLlYb/cLPe3+THSYtZvS+ZRTTmlZe/fdd4vrnnPOOcX6IYccUqzjmziyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLPXYOHChcX6McccU6w//PDDxfppp522zz3V5Z133inWr7jiimK93Vh6ydVXX93xutgbR3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9il68cUXW9Z27dpVXPftt98u1m+99dZi/ZprrinWu3HfffcV6xs3bizWS78L387ll19erF988cUdvzf21vbIbvto23+wvd72W7ZvqZbPsf287fer+8N73y6ATk3lY/wXkm6NiL+V9PeSbrR9vKTbJa2JiOMkrameAxhSbcMeEdsi4vXq8SeS1ks6StKFklZVL1sl6aIe9QigBvt0gs72AknfkfQnSfMiYps0/h+CpCNbrDNqu2m7OTY21mW7ADo15bDbPljSk5J+GhF/nep6EbEyIhoR0Wj3w4sAemdKYbe9v8aD/puIeKpavN32/Ko+X9KO3rQIoA5th95sW9IjktZHxC8nlJ6RdJWkZdX90z3pcEjMnj27ZW18F7UWEcX6K6+80lW9l7766qtifb/9yseLa6+9tmXtrrvuKq47cyYjw3Wayt48XdKPJa21/Ua17GcaD/nvbF8n6c+SftiTDgHUom3YI+KPklodur5fbzsAeoXLZYEkCDuQBGEHkiDsQBKEHUiCgcwpKv2cc7spl9v9CewwO/jgg4v1xYsXF+tLly5tWZs1a1ZHPaEzHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2WuwYcOGYv2hhx4q1lesWFGsb926dZ97+tr1119frF9yySXFeqPRKNYPPfTQfe4Jg8GRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeScLvfNK9To9GIZrPZt+0B2TQaDTWbzUl/DZojO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4k0Tbsto+2/Qfb622/ZfuWavmdtrfafqO6XdD7dgF0aio/XvGFpFsj4nXbh0h6zfbzVe1XEfGvvWsPQF2mMj/7Nknbqsef2F4v6aheNwagXvv0nd32AknfkfSnatEi22/aftT24S3WGbXdtN0cGxvrrlsAHZty2G0fLOlJST+NiL9KelDStyWdrPEj/y8mWy8iVkZEIyIaIyMj3XcMoCNTCrvt/TUe9N9ExFOSFBHbI+LLiPhK0kOSTu1dmwC6NZWz8Zb0iKT1EfHLCcvnT3jZDyStq789AHWZytn40yX9WNJa229Uy34m6VLbJ0sKSZsk/aQH/QGoyVTOxv9R0mR/H/tc/e0A6BWuoAOSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTR1ymbbY9J2jxh0VxJH/WtgX0zrL0Na18SvXWqzt6OiYhJf/+tr2Hfa+N2MyIaA2ugYFh7G9a+JHrrVL9642M8kARhB5IYdNhXDnj7JcPa27D2JdFbp/rS20C/swPon0Ef2QH0CWEHkhhI2G2fZ/td2xts3z6IHlqxvcn22moa6uaAe3nU9g7b6yYsm2P7edvvV/eTzrE3oN6GYhrvwjTjA913g57+vO/f2W3PkPSepH+UtEXSq5IujYi3+9pIC7Y3SWpExMAvwLD9PUmfSnosIv6uWvYvknZGxLLqP8rDI+K2IentTkmfDnoa72q2ovkTpxmXdJGkqzXAfVfo65/Uh/02iCP7qZI2RMQHEfG5pN9KunAAfQy9iHhJ0s49Fl8oaVX1eJXG/7H0XYvehkJEbIuI16vHn0j6eprxge67Ql99MYiwHyXpLxOeb9Fwzfcekn5v+zXbo4NuZhLzImKbNP6PR9KRA+5nT22n8e6nPaYZH5p918n0590aRNgnm0pqmMb/To+I70o6X9KN1cdVTM2UpvHul0mmGR8KnU5/3q1BhH2LpKMnPP+WpA8H0MekIuLD6n6HpNUavqmot389g251v2PA/fy/YZrGe7JpxjUE+26Q058PIuyvSjrO9rG2Z0n6kaRnBtDHXmwfVJ04ke2DJJ2j4ZuK+hlJV1WPr5L09AB7+YZhmca71TTjGvC+G/j05xHR95ukCzR+Rv5/JP3zIHpo0dffSPrv6vbWoHuT9LjGP9b9r8Y/EV0n6QhJayS9X93PGaLe/l3SWklvajxY8wfU2xka/2r4pqQ3qtsFg953hb76st+4XBZIgivogCQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJ/wND2RWJxKtXVgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 테스트 데이터를 사용하여 모델을 테스트한다.\n",
    "with torch.no_grad(): # torch.no_grad()를 하면 gradient 계산을 수행하지 않는다.\n",
    "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # MNIST 테스트 데이터에서 무작위로 하나를 뽑아서 예측을 해본다\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1,1, 28, 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "\n",
    "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN vs. deep CNN\n",
    "- Deep layers does NOT guarantee the enhancement of accuracy.\n",
    "- LR scheduling works fine enough to get higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "692b612c31f58e434f61d13ae189024361a32076fcfcdbea6d81ac48a7953dd4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
