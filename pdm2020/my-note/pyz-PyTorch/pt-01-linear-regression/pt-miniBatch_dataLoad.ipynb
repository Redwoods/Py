{"cells":[{"cell_type":"markdown","metadata":{"id":"QeTKQ1UzCi8y"},"source":["# 미니 배치와 데이터 로드\n","## (Mini Batch and Data Load)"]},{"cell_type":"markdown","metadata":{},"source":["## 데이터 로드하기(Data Load)\n","- 데이터셋(Dataset)\n","- 데이터로더(DataLoader)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["'1.9.1+cu111'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","torch.__version__"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from torch.utils.data import TensorDataset # 텐서데이터셋\n","from torch.utils.data import DataLoader # 데이터로더"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x1e0f8dfab30>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# 랜덤 시드(random seed)를 줍니다.\n","torch.manual_seed(1)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":509,"status":"ok","timestamp":1623381604735,"user":{"displayName":"Sang Hoon Yi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghapo6hvSHc9JAMzT-d672q7BAVzrSC4-O1x0-G=s64","userId":"09983074769501817983"},"user_tz":-540},"id":"HAM5RsvtCc2D"},"outputs":[],"source":["# 데이터\n","x_train  =  torch.FloatTensor([[73,  80,  75], \n","                               [93,  88,  93], \n","                               [89,  91,  90], \n","                               [96,  98,  100],   \n","                               [73,  66,  70]])  \n","y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Build dataset\n","dataset = TensorDataset(x_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["### dataset -> dataloader\n","- 데이터셋을 만들었다면 데이터로더를 사용 가능합니다. \n","\n","- 데이터로더는 기본적으로 2개의 인자를 입력받는다. \n","    - 하나는 데이터셋 \n","    - 미니 배치의 크기입니다. \n","    \n","- 미니 배치의 크기는 통상적으로 2의 배수를 사용합니다."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class MultivariateLinearRegressionModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n","\n","    def forward(self, x):\n","        return self.linear(x)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parameter containing:\n","tensor([[-0.5435,  0.3462, -0.1188]], requires_grad=True), Parameter containing:\n","tensor([0.2937], requires_grad=True)]\n"]}],"source":["model = MultivariateLinearRegressionModel()\n","print(list(model.parameters()))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch    0/2000 Cost: 31667.599609\n","Epoch  100/2000 Cost: 0.225993\n","Epoch  200/2000 Cost: 0.223911\n","Epoch  300/2000 Cost: 0.221941\n","Epoch  400/2000 Cost: 0.220059\n","Epoch  500/2000 Cost: 0.218271\n","Epoch  600/2000 Cost: 0.216575\n","Epoch  700/2000 Cost: 0.214950\n","Epoch  800/2000 Cost: 0.213413\n","Epoch  900/2000 Cost: 0.211952\n","Epoch 1000/2000 Cost: 0.210559\n","Epoch 1100/2000 Cost: 0.209230\n","Epoch 1200/2000 Cost: 0.207967\n","Epoch 1300/2000 Cost: 0.206762\n","Epoch 1400/2000 Cost: 0.205618\n","Epoch 1500/2000 Cost: 0.204529\n","Epoch 1600/2000 Cost: 0.203481\n","Epoch 1700/2000 Cost: 0.202486\n","Epoch 1800/2000 Cost: 0.201539\n","Epoch 1900/2000 Cost: 0.200634\n","Epoch 2000/2000 Cost: 0.199770\n"]}],"source":["# Learning using the whole batch\n","nb_epochs = 2000\n","for epoch in range(nb_epochs+1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train) # forward()\n","    # model(x_train)은 model.forward(x_train)와 동일함.\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward()\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","    # 100번마다 로그 출력\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.2306]], grad_fn=<AddmmBackward>)\n"]}],"source":["# 임의의 입력 [73, 80, 75]를 선언\n","new_var =  torch.FloatTensor([[73, 80, 75]]) \n","# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var) \n","print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "]},{"cell_type":"markdown","metadata":{},"source":["---\n","### Learning using mini-batch from dataloader\n","---"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parameter containing:\n","tensor([[ 0.1373,  0.4789, -0.2398]], requires_grad=True), Parameter containing:\n","tensor([-0.2437], requires_grad=True)]\n"]}],"source":["#  Initialize a model and an optimizer\n","model = MultivariateLinearRegressionModel()\n","print(list(model.parameters()))\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","[tensor([[ 96.,  98., 100.],\n","        [ 73.,  66.,  70.]]), tensor([[196.],\n","        [142.]])]\n","Epoch    0/20 Batch 1/3 Cost: 19709.205078\n","1\n","[tensor([[73., 80., 75.],\n","        [93., 88., 93.]]), tensor([[152.],\n","        [185.]])]\n","Epoch    0/20 Batch 2/3 Cost: 6080.016602\n","2\n","[tensor([[89., 91., 90.]]), tensor([[180.]])]\n","Epoch    0/20 Batch 3/3 Cost: 2136.703125\n","0\n","[tensor([[73., 80., 75.],\n","        [93., 88., 93.]]), tensor([[152.],\n","        [185.]])]\n","Epoch    1/20 Batch 1/3 Cost: 581.063416\n","1\n","[tensor([[89., 91., 90.],\n","        [73., 66., 70.]]), tensor([[180.],\n","        [142.]])]\n","Epoch    1/20 Batch 2/3 Cost: 185.738266\n","2\n","[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n","Epoch    1/20 Batch 3/3 Cost: 77.145729\n","0\n","[tensor([[ 96.,  98., 100.],\n","        [ 89.,  91.,  90.]]), tensor([[196.],\n","        [180.]])]\n","Epoch    2/20 Batch 1/3 Cost: 9.839544\n","1\n","[tensor([[73., 66., 70.],\n","        [93., 88., 93.]]), tensor([[142.],\n","        [185.]])]\n","Epoch    2/20 Batch 2/3 Cost: 26.419914\n","2\n","[tensor([[73., 80., 75.]]), tensor([[152.]])]\n","Epoch    2/20 Batch 3/3 Cost: 6.653654\n","0\n","[tensor([[ 73.,  66.,  70.],\n","        [ 96.,  98., 100.]]), tensor([[142.],\n","        [196.]])]\n","Epoch    3/20 Batch 1/3 Cost: 8.586456\n","1\n","[tensor([[73., 80., 75.],\n","        [89., 91., 90.]]), tensor([[152.],\n","        [180.]])]\n","Epoch    3/20 Batch 2/3 Cost: 3.945974\n","2\n","[tensor([[93., 88., 93.]]), tensor([[185.]])]\n","Epoch    3/20 Batch 3/3 Cost: 15.504026\n","0\n","[tensor([[73., 80., 75.],\n","        [93., 88., 93.]]), tensor([[152.],\n","        [185.]])]\n","Epoch    4/20 Batch 1/3 Cost: 7.821625\n","1\n","[tensor([[ 89.,  91.,  90.],\n","        [ 96.,  98., 100.]]), tensor([[180.],\n","        [196.]])]\n","Epoch    4/20 Batch 2/3 Cost: 3.242252\n","2\n","[tensor([[73., 66., 70.]]), tensor([[142.]])]\n","Epoch    4/20 Batch 3/3 Cost: 11.014041\n","0\n","[tensor([[ 96.,  98., 100.],\n","        [ 89.,  91.,  90.]]), tensor([[196.],\n","        [180.]])]\n","Epoch    5/20 Batch 1/3 Cost: 4.735202\n","1\n","[tensor([[73., 66., 70.],\n","        [93., 88., 93.]]), tensor([[142.],\n","        [185.]])]\n","Epoch    5/20 Batch 2/3 Cost: 9.256576\n","2\n","[tensor([[73., 80., 75.]]), tensor([[152.]])]\n","Epoch    5/20 Batch 3/3 Cost: 14.129298\n","0\n","[tensor([[ 96.,  98., 100.],\n","        [ 89.,  91.,  90.]]), tensor([[196.],\n","        [180.]])]\n","Epoch    6/20 Batch 1/3 Cost: 0.832383\n","1\n","[tensor([[93., 88., 93.],\n","        [73., 66., 70.]]), tensor([[185.],\n","        [142.]])]\n","Epoch    6/20 Batch 2/3 Cost: 12.838350\n","2\n","[tensor([[73., 80., 75.]]), tensor([[152.]])]\n","Epoch    6/20 Batch 3/3 Cost: 11.851681\n","0\n","[tensor([[89., 91., 90.],\n","        [93., 88., 93.]]), tensor([[180.],\n","        [185.]])]\n","Epoch    7/20 Batch 1/3 Cost: 6.192510\n","1\n","[tensor([[ 73.,  80.,  75.],\n","        [ 96.,  98., 100.]]), tensor([[152.],\n","        [196.]])]\n","Epoch    7/20 Batch 2/3 Cost: 3.968405\n","2\n","[tensor([[73., 66., 70.]]), tensor([[142.]])]\n","Epoch    7/20 Batch 3/3 Cost: 13.360912\n","0\n","[tensor([[73., 80., 75.],\n","        [73., 66., 70.]]), tensor([[152.],\n","        [142.]])]\n","Epoch    8/20 Batch 1/3 Cost: 8.720755\n","1\n","[tensor([[89., 91., 90.],\n","        [93., 88., 93.]]), tensor([[180.],\n","        [185.]])]\n","Epoch    8/20 Batch 2/3 Cost: 4.782454\n","2\n","[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n","Epoch    8/20 Batch 3/3 Cost: 1.505841\n","0\n","[tensor([[73., 66., 70.],\n","        [89., 91., 90.]]), tensor([[142.],\n","        [180.]])]\n","Epoch    9/20 Batch 1/3 Cost: 6.137552\n","1\n","[tensor([[ 93.,  88.,  93.],\n","        [ 96.,  98., 100.]]), tensor([[185.],\n","        [196.]])]\n","Epoch    9/20 Batch 2/3 Cost: 3.829573\n","2\n","[tensor([[73., 80., 75.]]), tensor([[152.]])]\n","Epoch    9/20 Batch 3/3 Cost: 10.299246\n","0\n","[tensor([[93., 88., 93.],\n","        [73., 66., 70.]]), tensor([[185.],\n","        [142.]])]\n","Epoch   10/20 Batch 1/3 Cost: 12.954241\n","1\n","[tensor([[73., 80., 75.],\n","        [89., 91., 90.]]), tensor([[152.],\n","        [180.]])]\n","Epoch   10/20 Batch 2/3 Cost: 8.711867\n","2\n","[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n","Epoch   10/20 Batch 3/3 Cost: 0.017977\n","0\n","[tensor([[73., 66., 70.],\n","        [89., 91., 90.]]), tensor([[142.],\n","        [180.]])]\n","Epoch   11/20 Batch 1/3 Cost: 6.677989\n","1\n","[tensor([[ 73.,  80.,  75.],\n","        [ 96.,  98., 100.]]), tensor([[152.],\n","        [196.]])]\n","Epoch   11/20 Batch 2/3 Cost: 3.517792\n","2\n","[tensor([[93., 88., 93.]]), tensor([[185.]])]\n","Epoch   11/20 Batch 3/3 Cost: 12.888184\n","0\n","[tensor([[ 73.,  80.,  75.],\n","        [ 96.,  98., 100.]]), tensor([[152.],\n","        [196.]])]\n","Epoch   12/20 Batch 1/3 Cost: 7.695931\n","1\n","[tensor([[73., 66., 70.],\n","        [93., 88., 93.]]), tensor([[142.],\n","        [185.]])]\n","Epoch   12/20 Batch 2/3 Cost: 9.697113\n","2\n","[tensor([[89., 91., 90.]]), tensor([[180.]])]\n","Epoch   12/20 Batch 3/3 Cost: 7.431065\n","0\n","[tensor([[ 96.,  98., 100.],\n","        [ 89.,  91.,  90.]]), tensor([[196.],\n","        [180.]])]\n","Epoch   13/20 Batch 1/3 Cost: 1.078431\n","1\n","[tensor([[93., 88., 93.],\n","        [73., 66., 70.]]), tensor([[185.],\n","        [142.]])]\n","Epoch   13/20 Batch 2/3 Cost: 12.258745\n","2\n","[tensor([[73., 80., 75.]]), tensor([[152.]])]\n","Epoch   13/20 Batch 3/3 Cost: 11.916381\n","0\n","[tensor([[89., 91., 90.],\n","        [73., 66., 70.]]), tensor([[180.],\n","        [142.]])]\n","Epoch   14/20 Batch 1/3 Cost: 6.649107\n","1\n","[tensor([[ 96.,  98., 100.],\n","        [ 93.,  88.,  93.]]), tensor([[196.],\n","        [185.]])]\n","Epoch   14/20 Batch 2/3 Cost: 4.356503\n","2\n","[tensor([[73., 80., 75.]]), tensor([[152.]])]\n","Epoch   14/20 Batch 3/3 Cost: 9.545249\n","0\n","[tensor([[73., 66., 70.],\n","        [89., 91., 90.]]), tensor([[142.],\n","        [180.]])]\n","Epoch   15/20 Batch 1/3 Cost: 7.157655\n","1\n","[tensor([[ 96.,  98., 100.],\n","        [ 73.,  80.,  75.]]), tensor([[196.],\n","        [152.]])]\n","Epoch   15/20 Batch 2/3 Cost: 3.042017\n","2\n","[tensor([[93., 88., 93.]]), tensor([[185.]])]\n","Epoch   15/20 Batch 3/3 Cost: 13.480201\n","0\n","[tensor([[89., 91., 90.],\n","        [73., 66., 70.]]), tensor([[180.],\n","        [142.]])]\n","Epoch   16/20 Batch 1/3 Cost: 5.911246\n","1\n","[tensor([[ 93.,  88.,  93.],\n","        [ 96.,  98., 100.]]), tensor([[185.],\n","        [196.]])]\n","Epoch   16/20 Batch 2/3 Cost: 3.069077\n","2\n","[tensor([[73., 80., 75.]]), tensor([[152.]])]\n","Epoch   16/20 Batch 3/3 Cost: 11.771445\n","0\n","[tensor([[ 93.,  88.,  93.],\n","        [ 96.,  98., 100.]]), tensor([[185.],\n","        [196.]])]\n","Epoch   17/20 Batch 1/3 Cost: 5.509836\n","1\n","[tensor([[89., 91., 90.],\n","        [73., 80., 75.]]), tensor([[180.],\n","        [152.]])]\n","Epoch   17/20 Batch 2/3 Cost: 5.962376\n","2\n","[tensor([[73., 66., 70.]]), tensor([[142.]])]\n","Epoch   17/20 Batch 3/3 Cost: 13.497574\n","0\n","[tensor([[ 96.,  98., 100.],\n","        [ 73.,  66.,  70.]]), tensor([[196.],\n","        [142.]])]\n","Epoch   18/20 Batch 1/3 Cost: 4.214231\n","1\n","[tensor([[73., 80., 75.],\n","        [89., 91., 90.]]), tensor([[152.],\n","        [180.]])]\n","Epoch   18/20 Batch 2/3 Cost: 8.234727\n","2\n","[tensor([[93., 88., 93.]]), tensor([[185.]])]\n","Epoch   18/20 Batch 3/3 Cost: 10.703908\n","0\n","[tensor([[ 96.,  98., 100.],\n","        [ 93.,  88.,  93.]]), tensor([[196.],\n","        [185.]])]\n","Epoch   19/20 Batch 1/3 Cost: 3.040166\n","1\n","[tensor([[73., 80., 75.],\n","        [73., 66., 70.]]), tensor([[152.],\n","        [142.]])]\n","Epoch   19/20 Batch 2/3 Cost: 8.904734\n","2\n","[tensor([[89., 91., 90.]]), tensor([[180.]])]\n","Epoch   19/20 Batch 3/3 Cost: 5.290576\n","0\n","[tensor([[93., 88., 93.],\n","        [73., 66., 70.]]), tensor([[185.],\n","        [142.]])]\n","Epoch   20/20 Batch 1/3 Cost: 10.663209\n","1\n","[tensor([[ 89.,  91.,  90.],\n","        [ 96.,  98., 100.]]), tensor([[180.],\n","        [196.]])]\n","Epoch   20/20 Batch 2/3 Cost: 4.885416\n","2\n","[tensor([[73., 80., 75.]]), tensor([[152.]])]\n","Epoch   20/20 Batch 3/3 Cost: 6.898077\n"]}],"source":["# Learning using mini-batch from dataloader\n","nb_epochs = 20\n","for epoch in range(nb_epochs + 1):\n","  for batch_idx, samples in enumerate(dataloader):\n","    print(batch_idx)\n","    print(samples)\n","    x_train, y_train = samples\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train)\n","\n","    # cost로 H(x) 계산\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n","        epoch, nb_epochs, batch_idx+1, len(dataloader),\n","        cost.item()\n","        ))"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[153.7148]], grad_fn=<AddmmBackward>)\n"]}],"source":["# 임의의 입력 [73, 80, 75]를 선언\n","new_var =  torch.FloatTensor([[73, 80, 75]]) \n","# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var) \n","print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parameter containing:\n","tensor([[0.6858, 1.0082, 0.3098]], requires_grad=True), Parameter containing:\n","tensor([-0.2374], requires_grad=True)]\n"]}],"source":["print(list(model.parameters()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPbZME4II7B55kkhgCe9EYp","collapsed_sections":[],"name":"auto-grad.ipynb","provenance":[]},"interpreter":{"hash":"692b612c31f58e434f61d13ae189024361a32076fcfcdbea6d81ac48a7953dd4"},"kernelspec":{"display_name":"Python 3.8.5 64-bit ('tf2': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
