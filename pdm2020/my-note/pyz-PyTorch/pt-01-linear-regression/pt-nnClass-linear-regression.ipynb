{"cells":[{"cell_type":"markdown","metadata":{"id":"QeTKQ1UzCi8y"},"source":["# 클래스로 파이토치 모델 구현: 선형 회귀"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","torch.__version__"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x20f045d8c50>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# 랜덤 시드(random seed)를 줍니다.\n","torch.manual_seed(1)"]},{"cell_type":"markdown","metadata":{},"source":["### linear regression"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":509,"status":"ok","timestamp":1623381604735,"user":{"displayName":"Sang Hoon Yi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghapo6hvSHc9JAMzT-d672q7BAVzrSC4-O1x0-G=s64","userId":"09983074769501817983"},"user_tz":-540},"id":"HAM5RsvtCc2D"},"outputs":[],"source":["# 데이터\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# class LinearRegressionModel\n","class LinearRegressionModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(1, 1)\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","# model = LinearRegressionModel()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["model = LinearRegressionModel()\n","# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01) "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parameter containing:\n","tensor([[-0.1939]], requires_grad=True), Parameter containing:\n","tensor([0.4694], requires_grad=True)]\n"]}],"source":["print(list(model.parameters()))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch    0/2000 Cost: 18.562185\n","Epoch  100/2000 Cost: 0.128051\n","Epoch  200/2000 Cost: 0.079128\n","Epoch  300/2000 Cost: 0.048896\n","Epoch  400/2000 Cost: 0.030215\n","Epoch  500/2000 Cost: 0.018671\n","Epoch  600/2000 Cost: 0.011538\n","Epoch  700/2000 Cost: 0.007129\n","Epoch  800/2000 Cost: 0.004406\n","Epoch  900/2000 Cost: 0.002722\n","Epoch 1000/2000 Cost: 0.001682\n","Epoch 1100/2000 Cost: 0.001040\n","Epoch 1200/2000 Cost: 0.000642\n","Epoch 1300/2000 Cost: 0.000397\n","Epoch 1400/2000 Cost: 0.000245\n","Epoch 1500/2000 Cost: 0.000152\n","Epoch 1600/2000 Cost: 0.000094\n","Epoch 1700/2000 Cost: 0.000058\n","Epoch 1800/2000 Cost: 0.000036\n","Epoch 1900/2000 Cost: 0.000022\n","Epoch 2000/2000 Cost: 0.000014\n"]}],"source":["# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n","nb_epochs = 2000\n","for epoch in range(nb_epochs+1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward() # backward 연산\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","    # 100번마다 로그 출력\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))"]},{"cell_type":"markdown","metadata":{},"source":["### test of linear model"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"hMQilCWkDshQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["훈련 후 입력이 4일 때의 예측값 : tensor([[7.9926]], grad_fn=<AddmmBackward>)\n"]}],"source":["# 임의의 입력 4를 선언\n","new_var =  torch.FloatTensor([[4.0]]) \n","# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var) # forward 연산\n","# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n","print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parameter containing:\n","tensor([[1.9957]], requires_grad=True), Parameter containing:\n","tensor([0.0097], requires_grad=True)]\n"]}],"source":["# 학습 후의 W와 b의 값을 출력해보겠습니다.\n","\n","print(list(model.parameters()))"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{},"source":["## Multivariable linear regression"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x20f045d8c50>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(1)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# 데이터\n","x_train = torch.FloatTensor([[73, 80, 75],\n","                             [93, 88, 93],\n","                             [89, 91, 90],\n","                             [96, 98, 100],\n","                             [73, 66, 70]])\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["class MultivariateLinearRegressionModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n","\n","    def forward(self, x):\n","        return self.linear(x)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parameter containing:\n","tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n","tensor([0.2710], requires_grad=True)]\n"]}],"source":["model2 = MultivariateLinearRegressionModel()\n","print(list(model2.parameters())) # initial parameters"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch    0/2000 Cost: 31667.599609\n","Epoch  100/2000 Cost: 0.225993\n","Epoch  200/2000 Cost: 0.223911\n","Epoch  300/2000 Cost: 0.221941\n","Epoch  400/2000 Cost: 0.220059\n","Epoch  500/2000 Cost: 0.218271\n","Epoch  600/2000 Cost: 0.216575\n","Epoch  700/2000 Cost: 0.214950\n","Epoch  800/2000 Cost: 0.213413\n","Epoch  900/2000 Cost: 0.211952\n","Epoch 1000/2000 Cost: 0.210559\n","Epoch 1100/2000 Cost: 0.209230\n","Epoch 1200/2000 Cost: 0.207967\n","Epoch 1300/2000 Cost: 0.206762\n","Epoch 1400/2000 Cost: 0.205618\n","Epoch 1500/2000 Cost: 0.204529\n","Epoch 1600/2000 Cost: 0.203481\n","Epoch 1700/2000 Cost: 0.202486\n","Epoch 1800/2000 Cost: 0.201539\n","Epoch 1900/2000 Cost: 0.200634\n","Epoch 2000/2000 Cost: 0.199770\n"]}],"source":["optimizer = torch.optim.SGD(model2.parameters(), lr=1e-5) \n","\n","nb_epochs = 2000\n","for epoch in range(nb_epochs+1):\n","\n","    # H(x) 계산\n","    prediction = model2(x_train)\n","    # model(x_train)은 model.forward(x_train)와 동일함.\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward()\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","    # 100번마다 로그 출력\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.2306]], grad_fn=<AddmmBackward>)\n"]}],"source":["# 임의의 입력 [73, 80, 75]를 선언\n","new_var =  torch.FloatTensor([[73, 80, 75]]) \n","# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model2(new_var) \n","print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parameter containing:\n","tensor([[0.9778, 0.4539, 0.5768]], requires_grad=True), Parameter containing:\n","tensor([0.2802], requires_grad=True)]\n"]}],"source":["print(list(model2.parameters())) # trained parameters"]},{"cell_type":"markdown","metadata":{},"source":["---"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPbZME4II7B55kkhgCe9EYp","collapsed_sections":[],"name":"auto-grad.ipynb","provenance":[]},"interpreter":{"hash":"692b612c31f58e434f61d13ae189024361a32076fcfcdbea6d81ac48a7953dd4"},"kernelspec":{"display_name":"Python 3.8.5 64-bit ('tf2': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
