{"cells":[{"cell_type":"markdown","metadata":{"id":"QeTKQ1UzCi8y"},"source":["# nn.Module로 구현하는 선형 회귀"]},{"cell_type":"markdown","metadata":{},"source":["- 파이토치에서는 선형 회귀 모델이 nn.Linear()라는 함수로, 또 평균 제곱오차가 nn.functional.mse_loss()라는 함수로 구현되어져 있습니다. \n","- 아래는 이번 실습에서 사용할 두 함수의 사용 예제를 간단히 보여줍니다.\n","\n","```\n","import torch.nn as nn\n","model = nn.Linear(input_dim, output_dim)\n","```\n","\n","```\n","import torch.nn.functional as F\n","cost = F.mse_loss(prediction, y_train)\n","```"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","torch.__version__"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x22e36bc9c30>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# 랜덤 시드(random seed)를 줍니다.\n","torch.manual_seed(1)"]},{"cell_type":"markdown","metadata":{},"source":["### linear regression"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":509,"status":"ok","timestamp":1623381604735,"user":{"displayName":"Sang Hoon Yi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghapo6hvSHc9JAMzT-d672q7BAVzrSC4-O1x0-G=s64","userId":"09983074769501817983"},"user_tz":-540},"id":"HAM5RsvtCc2D"},"outputs":[],"source":["# 데이터\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parameter containing:\n","tensor([[0.5153]], requires_grad=True), Parameter containing:\n","tensor([-0.4414], requires_grad=True)]\n"]}],"source":["# 모델을 선언 및 초기화. 단순 선형 회귀이므로 input_dim=1, output_dim=1.\n","model = nn.Linear(1,1)  # one input to one output\n","print(list(model.parameters()))   # (w,b)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01) "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch    0/2000 Cost: 13.103541\n","Epoch  100/2000 Cost: 0.002791\n","Epoch  200/2000 Cost: 0.001724\n","Epoch  300/2000 Cost: 0.001066\n","Epoch  400/2000 Cost: 0.000658\n","Epoch  500/2000 Cost: 0.000407\n","Epoch  600/2000 Cost: 0.000251\n","Epoch  700/2000 Cost: 0.000155\n","Epoch  800/2000 Cost: 0.000096\n","Epoch  900/2000 Cost: 0.000059\n","Epoch 1000/2000 Cost: 0.000037\n","Epoch 1100/2000 Cost: 0.000023\n","Epoch 1200/2000 Cost: 0.000014\n","Epoch 1300/2000 Cost: 0.000009\n","Epoch 1400/2000 Cost: 0.000005\n","Epoch 1500/2000 Cost: 0.000003\n","Epoch 1600/2000 Cost: 0.000002\n","Epoch 1700/2000 Cost: 0.000001\n","Epoch 1800/2000 Cost: 0.000001\n","Epoch 1900/2000 Cost: 0.000000\n","Epoch 2000/2000 Cost: 0.000000\n"]}],"source":["# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n","nb_epochs = 2000\n","for epoch in range(nb_epochs+1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward() # backward 연산\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","    # 100번마다 로그 출력\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))"]},{"cell_type":"markdown","metadata":{},"source":["### test of linear model"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"hMQilCWkDshQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["훈련 후 입력이 4일 때의 예측값 : tensor([[7.9989]], grad_fn=<AddmmBackward>)\n"]}],"source":["# 임의의 입력 4를 선언\n","new_var =  torch.FloatTensor([[4.0]]) \n","# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var) # forward 연산\n","# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n","print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parameter containing:\n","tensor([[1.9994]], requires_grad=True), Parameter containing:\n","tensor([0.0014], requires_grad=True)]\n"]}],"source":["# 학습 후의 W와 b의 값을 출력해보겠습니다.\n","\n","print(list(model.parameters()))"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{},"source":["## Multivariable linear regression"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x22e36bc9c30>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(1)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# 데이터\n","x_train = torch.FloatTensor([[73, 80, 75],\n","                             [93, 88, 93],\n","                             [89, 91, 90],\n","                             [96, 98, 100],\n","                             [73, 66, 70]])\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n","model2 = nn.Linear(3,1)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parameter containing:\n","tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n","tensor([0.2710], requires_grad=True)]\n"]}],"source":["print(list(model2.parameters()))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch    0/2000 Cost: 31667.599609\n","Epoch  100/2000 Cost: 0.225993\n","Epoch  200/2000 Cost: 0.223911\n","Epoch  300/2000 Cost: 0.221941\n","Epoch  400/2000 Cost: 0.220059\n","Epoch  500/2000 Cost: 0.218271\n","Epoch  600/2000 Cost: 0.216575\n","Epoch  700/2000 Cost: 0.214950\n","Epoch  800/2000 Cost: 0.213413\n","Epoch  900/2000 Cost: 0.211952\n","Epoch 1000/2000 Cost: 0.210559\n","Epoch 1100/2000 Cost: 0.209230\n","Epoch 1200/2000 Cost: 0.207967\n","Epoch 1300/2000 Cost: 0.206762\n","Epoch 1400/2000 Cost: 0.205618\n","Epoch 1500/2000 Cost: 0.204529\n","Epoch 1600/2000 Cost: 0.203481\n","Epoch 1700/2000 Cost: 0.202486\n","Epoch 1800/2000 Cost: 0.201539\n","Epoch 1900/2000 Cost: 0.200634\n","Epoch 2000/2000 Cost: 0.199770\n"]}],"source":["optimizer = torch.optim.SGD(model2.parameters(), lr=1e-5) \n","\n","nb_epochs = 2000\n","for epoch in range(nb_epochs+1):\n","\n","    # H(x) 계산\n","    prediction = model2(x_train)\n","    # model(x_train)은 model.forward(x_train)와 동일함.\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward()\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","    # 100번마다 로그 출력\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.2306]], grad_fn=<AddmmBackward>)\n"]}],"source":["# 임의의 입력 [73, 80, 75]를 선언\n","new_var =  torch.FloatTensor([[73, 80, 75]]) \n","# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model2(new_var) \n","print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parameter containing:\n","tensor([[0.9778, 0.4539, 0.5768]], requires_grad=True), Parameter containing:\n","tensor([0.2802], requires_grad=True)]\n"]}],"source":["print(list(model2.parameters()))"]},{"cell_type":"markdown","metadata":{},"source":["---"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPbZME4II7B55kkhgCe9EYp","collapsed_sections":[],"name":"auto-grad.ipynb","provenance":[]},"interpreter":{"hash":"692b612c31f58e434f61d13ae189024361a32076fcfcdbea6d81ac48a7953dd4"},"kernelspec":{"display_name":"Python 3.8.5 64-bit ('tf2': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
